#!/usr/bin/env python3
"""
Consolidated Mock Header and Unit Test Generator with Direct g++ Compilation
This script:
1. Reads all non-system headers from source files
2. Creates consolidated mock headers in a single folder
3. Generates unit tests as <filename>_<method>.cpp with boundary and condition tests
4. Compiles tests directly using g++ (NO CMAKE!)
5. Runs tests and reports results
6. Uses Ollama AI to improve test generation logic (optional, with --use-ollama flag)
"""

import os
import re
import json
import subprocess
import argparse
from pathlib import Path
from typing import List, Dict, Set, Tuple
import glob
import sys

# Add parent directory to path to import config_reader
sys.path.insert(0, str(Path(__file__).parent.parent))
from config_reader import get_project_path, get_ollama_model


def is_ollama_available() -> bool:
    """Check if Ollama is available and running"""
    try:
        # Check if ollama command exists
        result = subprocess.run(
            ["which", "ollama"],
            capture_output=True,
            timeout=2
        )
        if result.returncode != 0:
            return False
        
        # Check if ollama is actually running by trying a quick list
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            timeout=5
        )
        return result.returncode == 0
    except Exception:
        return False


def call_ollama(prompt: str, model: str = "qwen2.5:0.5b") -> str:
    """Call Ollama API to get AI assistance for test generation"""
    try:
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt.encode(),
            capture_output=True,
            timeout=30
        )
        return result.stdout.decode('utf-8', errors='ignore').strip()
    except Exception as e:
        # If Ollama fails, return empty string (fallback to default generation)
        return ""

class HeaderAnalyzer:
    """Analyzes C++ headers to extract class and method information"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.source_dir = self.project_root / "src"
        self.include_dir = self.project_root / "inc"
        self.all_headers = {}
        self.system_headers = {'iostream', 'vector', 'string', 'map', 'set', 'thread',
                               'mutex', 'memory', 'algorithm', 'functional', 'chrono',
                               'fstream', 'sstream', 'cstdint', 'cstring', 'cstdlib',
                               'cstdio', 'cassert', 'cmath', 'cctype', 'climits'}
        # Primitive types that don't need includes
        self.primitive_types = {
            'int', 'long', 'short', 'char', 'bool', 'float', 'double', 
            'unsigned', 'signed', 'void', 'size_t', 'uint8_t', 'uint16_t', 
            'uint32_t', 'uint64_t', 'int8_t', 'int16_t', 'int32_t', 'int64_t',
            'wchar_t', 'char16_t', 'char32_t'
        }
        # Common typedef and template parameter names
        self.common_typedefs = {
            'result_type', 'value_type', 'size_type', 'difference_type',
            'pointer', 'reference', 'const_pointer', 'const_reference',
            'iterator', 'const_iterator', 'reverse_iterator', 'const_reverse_iterator',
            'key_type', 'mapped_type', 'allocator_type', 'state_type',
            'element_type', 'first_type', 'second_type'
        }
    
    def find_all_source_files(self) -> List[Path]:
        """Find all .cpp source files"""
        cpp_files = []
        for ext in ['*.cpp']:
            # First try the src directory
            if self.source_dir.exists():
                cpp_files.extend(self.source_dir.rglob(ext))
            # If no files found or src dir doesn't exist, search project root
            if not cpp_files:
                for cpp_file in self.project_root.glob(ext):
                    # Skip test files and contrib directories
                    if 'test' not in cpp_file.name.lower() and 'contrib' not in str(cpp_file):
                        cpp_files.append(cpp_file)
        return cpp_files
    
    def find_all_headers(self) -> List[Path]:
        """Find all .h and .hpp header files"""
        headers = []
        # Try src and inc directories first
        for directory in [self.source_dir, self.include_dir]:
            if directory.exists():
                headers.extend(directory.rglob('*.h'))
                headers.extend(directory.rglob('*.hpp'))
        # If no headers found, search project root
        if not headers:
            for h_file in self.project_root.glob('*.h'):
                # Skip test files and contrib directories
                if 'test' not in h_file.name.lower() and 'contrib' not in str(h_file):
                    headers.append(h_file)
            for hpp_file in self.project_root.glob('*.hpp'):
                # Skip test files and contrib directories  
                if 'test' not in hpp_file.name.lower() and 'contrib' not in str(hpp_file):
                    headers.append(hpp_file)
        return headers
    
    def extract_includes_from_file(self, file_path: Path) -> Set[str]:
        """Extract non-system includes from a file"""
        includes = set()
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                # Find all #include "..." statements
                for match in re.finditer(r'#include\s*"([^"]+)"', content):
                    header = match.group(1)
                    # Extract just the filename
                    header_name = os.path.basename(header)
                    if header_name not in self.system_headers:
                        includes.add(header_name)
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
        return includes
    
    def parse_classes_from_header(self, header_path: Path) -> List[Dict]:
        """Parse ALL class information from header file - enhanced for complex headers with multiple classes"""
        try:
            with open(header_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Remove comments to avoid false matches
            content_no_comments = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
            content_no_comments = re.sub(r'/\*.*?\*/', '', content_no_comments, flags=re.DOTALL)
            
            # Find all classes in the file (handle multiple classes)
            # Pattern handles: class ClassName, class MACRO ClassName, class ClassName : public Base
            class_pattern = r'class\s+(?:\w+\s+)?(\w+)(?:\s*:\s*(?:public|protected|private)\s+\w+(?:\s*,\s*(?:public|protected|private)\s+\w+)*)?\s*\{'
            class_matches = list(re.finditer(class_pattern, content_no_comments))
            
            if not class_matches:
                return []
            
            all_classes = []
            
            # Parse each substantial class
            for match in class_matches:
                class_name = match.group(1)
                class_start = match.start()
                start_pos = match.end()
                
                # Check if this is a real class definition (has some content after {)
                next_100_chars = content_no_comments[start_pos:start_pos+100]
                if not next_100_chars.strip() or next_100_chars.strip().startswith('};'):
                    continue  # Skip forward declarations or empty classes
                
                class_info = self._parse_single_class(content, content_no_comments, class_name, class_start, header_path)
                if class_info and class_info['methods']:
                    all_classes.append(class_info)
            
            return all_classes
        except Exception as e:
            print(f"Error parsing {header_path}: {e}")
            return []
    
    def _parse_single_class(self, original_content: str, content: str, class_name: str, class_start: int, header_path: Path) -> Dict:
        """Parse a single class from the header"""
        try:
            # Find the end of this class by balancing braces from class_start
            class_content_start = content.find('{', class_start)
            if class_content_start == -1:
                return None
            
            # Balance braces to find class end (with safety limit)
            brace_count = 0
            class_end = class_content_start
            max_search = min(class_content_start + 50000, len(content))  # Safety limit
            
            for i in range(class_content_start, max_search):
                char = content[i]
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        class_end = i
                        break
            
            if brace_count != 0:
                # Couldn't find matching brace, use next class or end
                next_class = content.find('class ', class_content_start + 1)
                if next_class > 0:
                    class_end = next_class
                else:
                    class_end = max_search
            
            # Extract just this class's content
            class_content = content[class_start:class_end+1]
            
            # Extract methods
            methods = []
            
            # Find public methods - look for content between public: and next access specifier or class end
            # Use positive lookahead to get all public sections (there might be multiple)
            public_sections = []
            for match in re.finditer(r'public:\s*(.*?)(?=private:|protected:|$|\};)', class_content, re.DOTALL):
                public_sections.append(match.group(1))
            
            for public_text in public_sections:
                # Simple approach: remove inline implementations using regex
                # Match patterns like: method() { ... } or method() override { ... }
                public_text_cleaned = re.sub(r'\{[^{}]*\}', '', public_text)
                # Handle nested braces (run twice for safety)
                public_text_cleaned = re.sub(r'\{[^{}]*\}', '', public_text_cleaned)
                
                # Now normalize whitespace
                public_text_normalized = re.sub(r'\s+', ' ', public_text_cleaned)
                
                # Enhanced method patterns that handle more cases
                # Pattern for constructors: ClassName ( params ) optional-specifiers ;
                constructor_pattern = rf'{class_name}\s*\(([^)]*)\)\s*(?::|;)'
                
                # Pattern for destructors
                destructor_pattern = rf'~{class_name}\s*\(\s*\)\s*(?:{{|;|override)'
                
                # Enhanced pattern for regular methods
                # Captures: [virtual] [static] [explicit] [inline] return_type method_name ( params ) [const] [override] [= 0] ;
                # Group 1: modifiers (virtual/static/etc), Group 2: return_type, Group 3: method_name, Group 4: params
                method_pattern = r'((?:virtual\s+)?(?:static\s+)?(?:explicit\s+)?(?:inline\s+)?)(\w+(?:\s*\*|\s*&|\s*<[^>]+>)?)\s+(\w+)\s*\(([^)]*)\)\s*(?:const\s*)?(?:override\s*)?(?:=\s*0\s*)?(?:;|{)'
                
                # Pattern for auto return type methods
                auto_pattern = r'((?:static\s+)?)auto\s+(\w+)\s*\(([^)]*)\)\s*(?:const\s*)?->\s*(\w+(?:\s*\*|\s*&)?)\s*(?:;|{)'
                
                # Find constructors
                for match in re.finditer(constructor_pattern, public_text_normalized):
                    params = match.group(1).strip()
                    
                    # Parse parameters
                    param_list = self._parse_parameters(params)
                    
                    methods.append({
                        'name': class_name,
                        'return_type': '',
                        'parameters': param_list,
                        'is_constructor': True,
                        'is_destructor': False
                    })
                
                # Find destructors
                for match in re.finditer(destructor_pattern, public_text_normalized):
                    methods.append({
                        'name': f'~{class_name}',
                        'return_type': '',
                        'parameters': [],
                        'is_constructor': False,
                        'is_destructor': True
                    })
                
                # Find all regular methods
                for match in re.finditer(method_pattern, public_text_normalized):
                    modifiers = match.group(1).strip()
                    return_type = match.group(2).strip()
                    method_name = match.group(3)
                    params = match.group(4).strip()
                    
                    # Skip if this is actually a constructor (can happen with return type same as class name)
                    if method_name == class_name:
                        continue
                    
                    # Skip operators for now (they're complex)
                    if method_name.startswith('operator'):
                        continue
                    
                    # Check if static
                    is_static = 'static' in modifiers
                    
                    # Parse parameters
                    param_list = self._parse_parameters(params)
                    
                    methods.append({
                        'name': method_name,
                        'return_type': return_type,
                        'parameters': param_list,
                        'is_constructor': False,
                        'is_destructor': False,
                        'is_static': is_static
                    })
                
                # Find auto return type methods
                for match in re.finditer(auto_pattern, public_text_normalized):
                    modifiers = match.group(1).strip()
                    method_name = match.group(2)
                    params = match.group(3).strip()
                    return_type = match.group(4).strip()
                    
                    # Check if static
                    is_static = 'static' in modifiers
                    
                    # Parse parameters
                    param_list = self._parse_parameters(params)
                    
                    methods.append({
                        'name': method_name,
                        'return_type': return_type,
                        'parameters': param_list,
                        'is_constructor': False,
                        'is_destructor': False,
                        'is_static': is_static
                    })
            
            # Remove duplicate methods (can happen with multiple public sections)
            seen = set()
            unique_methods = []
            for method in methods:
                method_sig = f"{method['name']}_{len(method['parameters'])}"
                if method_sig not in seen:
                    seen.add(method_sig)
                    unique_methods.append(method)
            
            # Check if class is abstract (has pure virtual methods)
            is_abstract = '= 0' in class_content
            
            # Check if class has protected/private destructor (cannot be instantiated directly)
            # Look for protected: or private: followed by ~ClassName
            has_protected_destructor = False
            for match in re.finditer(r'(?:protected:|private:)\s*(.*?)(?=public:|private:|protected:|$|\};)', class_content, re.DOTALL):
                section_text = match.group(1)
                if f'~{class_name}' in section_text:
                    has_protected_destructor = True
                    break
            
            # Detect namespace by looking backward from class_start
            namespace = self._detect_namespace(content, class_start)
            
            return {
                'class_name': class_name,
                'methods': unique_methods,
                'header_file': header_path.name,
                'header_full_path': str(header_path),
                'is_abstract': is_abstract,
                'has_protected_destructor': has_protected_destructor,
                'namespace': namespace
            }
        except Exception as e:
            print(f"Error parsing class {class_name} in {header_path}: {e}")
            return None
    
    def _detect_namespace(self, content: str, class_start: int) -> str:
        """Detect the namespace that contains the class by looking backward from class_start
        Returns the namespace name or empty string if no namespace"""
        # Look backward from class_start to find namespace declaration
        before_class = content[:class_start]
        
        # Find all namespace declarations before this class
        # Pattern: namespace name {
        namespace_pattern = r'namespace\s+(\w+)\s*\{'
        namespaces = []
        
        for match in re.finditer(namespace_pattern, before_class):
            ns_name = match.group(1)
            ns_start = match.start()
            
            # Check if this namespace is still open at class_start
            # Count braces between namespace start and class start
            between = content[match.end():class_start]
            open_braces = between.count('{')
            close_braces = between.count('}')
            
            # If we have more opens than closes, the namespace is still open
            if open_braces >= close_braces:
                namespaces.append(ns_name)
        
        # Return the innermost namespace (last one found)
        return namespaces[-1] if namespaces else ""
    
    def _parse_parameters(self, params_str: str) -> list:
        """Helper method to parse parameter list from a string"""
        param_list = []
        if not params_str or params_str.strip() == 'void':
            return param_list
        
        # Handle parameters - split by comma but be careful of template parameters
        params = []
        current_param = []
        angle_bracket_depth = 0
        paren_depth = 0
        
        for char in params_str + ',':
            if char == '<':
                angle_bracket_depth += 1
                current_param.append(char)
            elif char == '>':
                angle_bracket_depth -= 1
                current_param.append(char)
            elif char == '(':
                paren_depth += 1
                current_param.append(char)
            elif char == ')':
                paren_depth -= 1
                current_param.append(char)
            elif char == ',' and angle_bracket_depth == 0 and paren_depth == 0:
                if current_param:
                    params.append(''.join(current_param).strip())
                    current_param = []
            else:
                current_param.append(char)
        
        for param in params:
            param = param.strip()
            if param:
                # Remove default values
                if '=' in param:
                    param = param.split('=')[0].strip()
                
                # Extract type and name - name is last token
                parts = param.split()
                if len(parts) >= 2:
                    # Last part is name, rest is type
                    name = parts[-1]
                    type_str = ' '.join(parts[:-1])
                    param_list.append({'type': type_str, 'name': name})
                elif len(parts) == 1:
                    # Just type, no name
                    param_list.append({'type': param, 'name': ''})
        
        return param_list
    
    def parse_class_from_header(self, header_path: Path) -> Dict:
        """Parse class information from header file - backward compatibility wrapper
        Returns the first/best class found, or dict with list of classes"""
        classes = self.parse_classes_from_header(header_path)
        if not classes:
            return None
        
        # Return the first class with most methods as the "main" class
        # This prioritizes more complete/important classes
        best_class = max(classes, key=lambda c: len(c['methods']))
        
        # Also add all classes to the result for multi-class support
        result = best_class.copy()
        result['classes'] = classes
        return result


class MockGenerator:
    """Generates mock headers for testing"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_mock_header(self, class_info: Dict) -> str:
        """Generate mock header content for a class"""
        class_name = class_info['class_name']
        guard = f"MOCK_{class_name.upper()}_H"
        
        content = f"""#ifndef {guard}
#define {guard}

// Mock header for {class_name}
// This is a simplified mock for testing purposes

"""
        
        # Add any necessary includes for types
        content += "#include <cstdint>\n"
        content += "#include <string>\n"
        
        # Check if we need common.h (for struct definitions)
        needs_common = False
        for method in class_info['methods']:
            for param in method['parameters']:
                if 'struct' in param['type'].lower():
                    needs_common = True
                    break
            if needs_common:
                break
        
        if needs_common:
            content += '#include "common.h"\n'
        
        content += "\n"
        
        # Generate class
        content += f"class {class_name} {{\n"
        content += "public:\n"
        
        # Always add default constructor and destructor
        content += f"    {class_name}() {{}}\n"
        content += f"    ~{class_name}() {{}}\n"
        
        for method in class_info['methods']:
            # Skip constructors and destructors as we added defaults above
            if method['is_constructor'] or method['is_destructor']:
                continue
            
            # Skip template methods or methods with malformed parameters
            # These often contain < > ( ) in the parameter type which indicates template or parsing error
            skip_method = False
            for p in method['parameters']:
                # Check if parameter type contains template syntax or casts
                if any(char in p['type'] for char in ['<', '>', '(', ')']):
                    skip_method = True
                    break
                # Check if parameter has no name and type looks invalid
                if not p['name'] and any(char in p['type'] for char in ['=', '+', '-', '*', '/']):
                    skip_method = True
                    break
            
            if skip_method:
                continue  # Skip this method - it's likely a template or has parsing issues
            
            # Normalize return type (fix common type names)
            return_type = method['return_type']
            if return_type == 'string':
                return_type = 'std::string'
            
            # Generate method signature
            params = ', '.join([f"{p['type']} {p['name']}" if p['name'] else p['type'] 
                               for p in method['parameters']])
            
            content += f"    {return_type} {method['name']}({params}) {{\n"
            # Generate default return value
            if 'void' not in return_type:
                if 'bool' in return_type:
                    content += "        return true;\n"
                elif 'int' in return_type or 'long' in return_type:
                    content += "        return 0;\n"
                elif '*' in return_type:
                    content += "        return nullptr;\n"
                elif '&' in return_type:
                    content += f"        static {return_type.replace('&', '')} dummy;\n"
                    content += "        return dummy;\n"
                else:
                    content += f"        return {return_type}();\n"
            content += "    }\n"
        
        content += "};\n\n"
        content += f"#endif // {guard}\n"
        
        return content
    
    def write_mock_header(self, class_info: Dict):
        """Write mock header to file"""
        if not class_info:
            return
        
        header_name = class_info['header_file']
        mock_content = self.generate_mock_header(class_info)
        
        output_path = self.output_dir / header_name
        with open(output_path, 'w') as f:
            f.write(mock_content)
        
        print(f"  Generated mock: {header_name}")


class UnitTestGenerator:
    """Generates unit tests for methods"""
    
    def __init__(self, output_dir: Path, mock_dir: Path, source_root: Path, use_ollama: bool = False):
        self.output_dir = output_dir
        self.mock_dir = mock_dir
        self.source_root = source_root
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create separate directory for Python-generated tests (backup)
        self.python_test_dir = output_dir.parent / "python_generated_tests"
        self.python_test_dir.mkdir(parents=True, exist_ok=True)
        
        self.test_metadata = []
        self.enhancement_plan = []  # Track what will be enhanced
        
        # Primitive types that don't need includes
        self.primitive_types = {
            'int', 'long', 'short', 'char', 'bool', 'float', 'double', 
            'unsigned', 'signed', 'void', 'size_t', 'uint8_t', 'uint16_t', 
            'uint32_t', 'uint64_t', 'int8_t', 'int16_t', 'int32_t', 'int64_t',
            'wchar_t', 'char16_t', 'char32_t'
        }
        
        # Common typedef and template parameter names that should not generate includes
        self.common_typedefs = {
            'result_type', 'value_type', 'size_type', 'difference_type',
            'pointer', 'reference', 'const_pointer', 'const_reference',
            'iterator', 'const_iterator', 'reverse_iterator', 'const_reverse_iterator',
            'key_type', 'mapped_type', 'allocator_type', 'state_type',
            'element_type', 'first_type', 'second_type'
        }
        
        # System headers that are standard library
        self.system_headers = {'iostream', 'vector', 'string', 'map', 'set', 'thread',
                               'mutex', 'memory', 'algorithm', 'functional', 'chrono',
                               'fstream', 'sstream', 'cstdint', 'cstring', 'cstdlib',
                               'cstdio', 'cassert', 'cmath', 'cctype', 'climits'}
        
        # Only check and use Ollama if explicitly requested
        self.ollama_available = False
        self.use_ollama = False
        
        if use_ollama:
            self.ollama_available = is_ollama_available()
            if self.ollama_available:
                print("  ðŸ¤– Ollama enabled - will use AI-enhanced test generation")
                print(f"  ðŸ“ Python-generated tests will be saved to: {self.python_test_dir}")
                self.use_ollama = True
            else:
                print("  âš ï¸  Ollama requested but not available - falling back to template-based generation")
        else:
            print("  ðŸ“ Using template-based test generation")
    
    def _has_init_method(self, class_info: Dict) -> bool:
        """Check if class has an init method"""
        for method in class_info['methods']:
            if method['name'] == 'init':
                return True
        return False
    
    def _has_close_method(self, class_info: Dict) -> bool:
        """Check if class has a close method"""
        for method in class_info['methods']:
            if method['name'] == 'close':
                return True
        return False
    
    def _get_constructor_info(self, class_info: Dict) -> Dict:
        """Get constructor information including required parameters
        Returns dict with 'has_params' and 'has_default' flags"""
        has_default = False
        parameterized_constructor = None
        
        for method in class_info['methods']:
            if method['is_constructor']:
                if not method['parameters'] or len(method['parameters']) == 0:
                    has_default = True
                else:
                    # Skip copy constructors and move constructors - we can't easily use them
                    # Copy constructor: ClassName(const ClassName&)
                    # Move constructor: ClassName(ClassName&&)
                    if len(method['parameters']) == 1:
                        param_type = method['parameters'][0]['type']
                        # Check if it's a self-referential constructor
                        if class_info['class_name'] in param_type:
                            continue
                    parameterized_constructor = method
        
        # If no explicit constructors found, class has implicit default constructor
        has_any_constructor = any(m['is_constructor'] for m in class_info['methods'])
        if not has_any_constructor:
            has_default = True
        
        # Prefer default constructor if available
        if has_default:
            return {'has_params': False, 'has_default': True, 'parameters': []}
        elif parameterized_constructor:
            return {
                'has_params': True,
                'has_default': False,
                'parameters': parameterized_constructor['parameters']
            }
        return {'has_params': False, 'has_default': has_default, 'parameters': []}
    
    def _generate_object_creation_code(self, class_name: str, class_info: Dict) -> tuple:
        """Generate code to create an object, handling constructor parameters
        Returns: (include_headers, object_declarations, object_creation)
        """
        constructor_info = self._get_constructor_info(class_info)
        
        if constructor_info['has_params']:
            # Need to create dependent objects first
            includes = []
            declarations = []
            params = []
            
            param_counter = {}  # Track parameter type counts for unique naming
            for param in constructor_info['parameters']:
                param_type = param['type'].replace('&', '').replace('const', '').replace('*', '').strip()
                
                # Skip empty parameter types
                if not param_type:
                    continue
                
                # Check if this is a primitive type or pointer/reference to primitive
                type_parts = param_type.split()
                base_type = type_parts[-1] if type_parts else param_type  # Get the actual type after qualifiers
                is_primitive = any(prim in base_type for prim in self.primitive_types)
                is_typedef = base_type in self.common_typedefs
                
                # NOTE: Don't generate includes for parameter types - they should be
                # defined in the header being tested or in its dependencies which are
                # already included. Generating includes like "MessageBuilder.h" causes
                # errors when the type is actually defined inline in the main header.
                
                # Create unique variable name
                # Handle namespace prefixes (std::, Catch::, etc.)
                if '::' in base_type:
                    # Extract just the class name after the last ::
                    class_only = base_type.split('::')[-1]
                    base_name_for_var = class_only.lower()[:5]
                elif is_primitive:
                    # For primitives, add a prefix to avoid using the type name as variable name
                    base_name_for_var = f"param_{base_type.lower()}"
                else:
                    base_name_for_var = base_type.lower()[:5]
                
                if base_name_for_var in param_counter:
                    param_counter[base_name_for_var] += 1
                    param_var = f"{base_name_for_var}{param_counter[base_name_for_var]}"
                else:
                    param_counter[base_name_for_var] = 1
                    param_var = base_name_for_var
                
                # Create instance of the parameter type (with default initialization for primitives/typedefs)
                if is_primitive or is_typedef:
                    # For primitives and typedefs, use simple initialization
                    if 'bool' in base_type:
                        declarations.append(f"    {param_type} {param_var} = false;")
                    elif any(t in base_type for t in ['int', 'long', 'short', 'char', 'size_t', 'type']):
                        declarations.append(f"    {param_type} {param_var} = 0;")
                    elif any(t in base_type for t in ['float', 'double']):
                        declarations.append(f"    {param_type} {param_var} = 0.0;")
                    else:
                        declarations.append(f"    {param_type} {param_var}{{}};")
                else:
                    declarations.append(f"    {param_type} {param_var};")
                params.append(param_var)
            
            obj_decl = f"    {class_name} obj({', '.join(params)});"
            
            return (includes, declarations, obj_decl)
        elif constructor_info['has_default']:
            # Has a default constructor or implicit default constructor
            return ([], [], f"    {class_name} obj;")
        else:
            # No default constructor and no parameterized constructor to use
            # Return None to signal that object creation is not possible
            return ([], [], None)
    
    def _validate_and_clean_ollama_response(self, response: str) -> str:
        """Validate and clean Ollama response by removing markdown artifacts and ensuring valid C++ code"""
        if not response:
            return ""
        
        # Strip markdown code fences if present
        # Remove ```cpp or ```c++ or ``` at the beginning
        response = re.sub(r'^```(?:cpp|c\+\+|c)?\s*\n?', '', response, flags=re.MULTILINE)
        # Remove ``` at the end
        response = re.sub(r'\n?```\s*$', '', response, flags=re.MULTILINE)
        # Remove any stray ``` in the middle
        response = response.replace('```', '')
        
        # Strip leading/trailing whitespace
        response = response.strip()
        
        # Validate that response contains TEST_F or TEST (for micro-tests)
        if 'TEST_F' not in response and 'TEST(' not in response:
            return ""
        
        # Extract only the TEST_F or TEST functions by finding matching braces
        test_functions = []
        pos = 0
        while pos < len(response):
            # Look for both TEST_F and TEST
            test_match = re.search(r'TEST(?:_F)?\s*\([^)]+\)\s*\{', response[pos:])
            if not test_match:
                break
            
            start_pos = pos + test_match.start()
            # Find the complete TEST function with balanced braces
            brace_count = 0
            in_test = False
            end_pos = start_pos
            
            for i in range(start_pos, len(response)):
                char = response[i]
                if char == '{':
                    brace_count += 1
                    in_test = True
                elif char == '}':
                    brace_count -= 1
                    if in_test and brace_count == 0:
                        end_pos = i + 1
                        test_functions.append(response[start_pos:end_pos])
                        pos = end_pos
                        break
            
            if end_pos == start_pos:
                # Couldn't find matching brace, skip
                break
        
        if test_functions:
            return '\n\n'.join(test_functions)
        
        # Fallback: use the whole response if it looks valid
        if ("TEST_F" in response or "TEST(" in response) and "{" in response and "}" in response:
            return response
        
        return ""
    
    def _plan_enhancement(self, class_name: str, method_name: str, method: Dict, 
                         has_init: bool, has_close: bool) -> str:
        """Generate a plan of what enhancements will be made"""
        enhancements = []
        
        if 'bool' in method['return_type']:
            enhancements.append("Add more specific boolean value assertions")
        elif 'int' in method['return_type'] or 'long' in method['return_type']:
            enhancements.append("Improve numeric boundary checks")
        elif 'void' in method['return_type']:
            enhancements.append("Add better exception handling validation")
        
        if has_init:
            enhancements.append("Verify proper initialization sequence")
        if has_close:
            enhancements.append("Ensure proper cleanup and resource management")
        
        enhancements.append("Improve assertion specificity (EXPECT vs ASSERT)")
        enhancements.append("Add edge case coverage")
        
        return f"{class_name}::{method_name} - " + ", ".join(enhancements)
    
    def _enhance_test_with_ollama_full(self, base_test_code: str, class_name: str, 
                                       method_name: str, method: Dict, has_init: bool, 
                                       has_close: bool) -> str:
        """Use Ollama to enhance Python-generated test code (returns full test or None)"""
        
        print(f"    ðŸ¤– Enhancing {class_name}::{method_name} with Ollama...", end=' ', flush=True)
        
        # Check if this is a micro-test (uses TEST) or regular test (uses TEST_F)
        is_micro_test = 'TEST(' in base_test_code and 'TEST_F' not in base_test_code
        
        # Extract just the TEST/TEST_F functions for enhancement
        test_start = base_test_code.find('// Test:') if not is_micro_test else base_test_code.find('TEST(')
        if test_start < 0:
            print("âŒ (No TEST found)")
            return None
        
        header_and_fixture = base_test_code[:test_start]
        test_functions = base_test_code[test_start:]
        
        test_type = "TEST" if is_micro_test else "TEST_F"
        
        prompt = f"""You are enhancing an existing C++ unit test. Review the test below and improve ONLY the test logic inside {test_type} functions.

Class: {class_name}
Method: {method_name}
Method signature: {method['return_type']} {method_name}()
Class has init(): {has_init}
Class has close(): {has_close}

Current Python-generated test code:
{test_functions}

Task: Enhance the {test_type} functions by:
1. Improving assertions to be more specific and meaningful
2. Adding better error checking (especially for init/close if they exist)
3. Ensuring proper object initialization sequence
4. Keeping all existing test cases but improving their logic
5. Using EXPECT instead of ASSERT for non-critical checks

Return ONLY the improved {test_type} functions (maintain all existing test names). Do NOT include headers, fixture definitions, or any other code. No explanations or markdown."""
        
        response = call_ollama(prompt)
        
        # Validate and clean the response
        cleaned_response = self._validate_and_clean_ollama_response(response)
        
        if not cleaned_response:
            # If Ollama fails or returns invalid code, return None (will use Python version)
            print("âŒ (Invalid response, using Python fallback)")
            return None
        
        print("âœ… (Enhanced)")
        # Combine header/fixture with enhanced test functions
        return header_and_fixture + cleaned_response + '\n'
    
    def _enhance_test_with_ollama(self, base_test_code: str, class_name: str, 
                                   method_name: str, method: Dict, has_init: bool, 
                                   has_close: bool) -> str:
        """Use Ollama to enhance Python-generated test code, not replace it (legacy method)"""
        
        prompt = f"""You are enhancing an existing C++ unit test. Review the test below and improve ONLY the test logic inside TEST_F functions.

Class: {class_name}
Method: {method_name}
Method signature: {method['return_type']} {method_name}()
Class has init(): {has_init}
Class has close(): {has_close}

Current Python-generated test code:
{base_test_code}

Task: Enhance the TEST_F functions by:
1. Improving assertions to be more specific and meaningful
2. Adding better error checking (especially for init/close if they exist)
3. Ensuring proper object initialization sequence
4. Keeping all existing test cases but improving their logic
5. Using EXPECT instead of ASSERT for non-critical checks

Return ONLY the improved TEST_F functions (maintain all existing test names). Do NOT include headers, fixture definitions, or any other code. No explanations or markdown."""
        
        response = call_ollama(prompt)
        
        # Validate and clean the response
        cleaned_response = self._validate_and_clean_ollama_response(response)
        
        if not cleaned_response:
            # If Ollama fails or returns invalid code, use the original
            return ""
        
        return cleaned_response
    
    def show_enhancement_plan(self):
        """Display the enhancement plan to the user"""
        if not self.enhancement_plan:
            return
        
        print("\n" + "="*70)
        print("ðŸ¤– OLLAMA ENHANCEMENT PLAN")
        print("="*70)
        print("\nThe following enhancements will be applied to each test:\n")
        for i, plan in enumerate(self.enhancement_plan, 1):
            print(f"{i}. {plan}")
        print("\n" + "="*70 + "\n")
    
    def generate_test_for_method(self, source_file: Path, class_info: Dict, method: Dict, 
                                  dependent_headers: Set[str]) -> str:
        """Generate unit test content for a specific method"""
        class_name = class_info['class_name']
        method_name = method['name']
        
        # Skip special methods for now
        if method['is_destructor']:
            return None
        
        # Determine the correct include path for the header
        # For Catch2 and other header-only libraries with subdirectory structure,
        # use the standard include format relative to the src directory
        header_file = class_info['header_file']
        header_full_path = Path(class_info.get('header_full_path', ''))
        
        if 'catch' in header_file.lower() and header_full_path.exists():
            # For Catch2, calculate the path relative to the src directory
            # E.g., /path/to/src/catch2/internal/catch_textflow.hpp -> catch2/internal/catch_textflow.hpp
            src_dir = self.source_root / 'src'
            try:
                rel_path = header_full_path.relative_to(src_dir)
                include_path = f"<{rel_path.as_posix()}>"
            except ValueError:
                # If relative_to fails, fall back to just the filename with catch2 prefix
                include_path = f"<catch2/{header_file}>"
        else:
            # For other projects, use quoted include
            include_path = f'"{header_file}"'
        
        content = f"""// Unit test for {class_name}::{method_name}
#include <gtest/gtest.h>
#include <climits>
#include <thread>
#include <chrono>


// Include actual header being tested (will use real implementation)
// For header-only libraries, this will automatically include all dependencies
#include {include_path}
"""
        
        # Add includes for constructor parameters if needed
        constructor_includes, _, _ = self._generate_object_creation_code(class_name, class_info)
        for inc in constructor_includes:
            if inc not in content:
                content += inc + '\n'
        
        # Add using namespace directive for Catch2 library when using real headers
        # Since we're now using real Catch2 headers (not mocks), the types are in namespace Catch
        if 'catch' in class_info['header_file'].lower():
            content += """
// Using Catch namespace since we're using real Catch2 headers
using namespace Catch;
"""
            # For TextFlow header specifically, also add TextFlow namespace
            if 'textflow' in class_info['header_file'].lower():
                content += """// Using TextFlow namespace for TextFlow classes
using namespace Catch::TextFlow;
"""
            content += "\n"
        
        content += f"""
// Test fixture for {class_name}::{method_name}
class {class_name}_{method_name}_Test : public ::testing::Test {{
protected:
    void SetUp() override {{
        // Setup code
    }}
    
    void TearDown() override {{
        // Cleanup code - ensure threads are stopped
        std::this_thread::sleep_for(std::chrono::milliseconds(50));
    }}
}};

"""
        
        # Generate tests based on method characteristics
        if method['is_constructor']:
            content += self._generate_constructor_tests(class_name, method_name, method, class_info)
        elif 'bool' in method['return_type']:
            content += self._generate_boolean_tests(class_name, method_name, method, class_info)
        elif 'int' in method['return_type'] or 'long' in method['return_type']:
            content += self._generate_numeric_tests(class_name, method_name, method, class_info)
        elif 'void' in method['return_type']:
            content += self._generate_void_tests(class_name, method_name, method, class_info)
        else:
            content += self._generate_generic_tests(class_name, method_name, method, class_info)
        
        # Add exception safety tests for non-constructor methods
        if not method['is_constructor'] and method_name not in ['init', 'close']:
            content += self._generate_exception_safety_tests(class_name, method_name, method, class_info)
        
        # Add stress tests only for methods that are likely to be called frequently
        if method_name not in ['init', 'close', 'run'] and not method['is_constructor']:
            content += self._generate_stress_tests(class_name, method_name, method, class_info)
        
        return content
    
    def _generate_constructor_tests(self, class_name: str, method_name: str, method: Dict, class_info: Dict) -> str:
        """Generate constructor tests"""
        _, obj_deps, obj_decl = self._generate_object_creation_code(class_name, class_info)
        obj_creation = '\n'.join(obj_deps) + '\n' + obj_decl if obj_deps else obj_decl
        
        content = f"""
// Test: Constructor creates valid object
TEST_F({class_name}_{method_name}_Test, ConstructorCreatesValidObject) {{
{obj_creation}
    // Object should be created without throwing
    SUCCEED();
}}
"""
        return content
    
    def _generate_boolean_tests(self, class_name: str, method_name: str, method: Dict, class_info: Dict) -> str:
        """Generate tests for boolean return methods"""
        
        # ALWAYS generate Python-based tests first
        decls, params = self._generate_param_values(method)
        
        # Get object creation code
        _, obj_deps, obj_decl = self._generate_object_creation_code(class_name, class_info)
        obj_creation = '\n'.join(obj_deps) + '\n' + obj_decl if obj_deps else obj_decl
        
        # Check if this class has init/close methods
        has_init = self._has_init_method(class_info) or method_name == 'init'
        has_close = self._has_close_method(class_info) or method_name == 'close'
        
        # Special handling for init method
        if method_name == 'init':
            close_call = "    obj.close();\n    std::this_thread::sleep_for(std::chrono::milliseconds(100));" if has_close else ""
            content = f"""
// Test: {method_name} returns true on success
TEST_F({class_name}_{method_name}_Test, ReturnsTrueOnSuccess) {{
{obj_creation}
{decls}
    bool result = obj.{method_name}({params});
    EXPECT_TRUE(result);
    
    // Give threads time to start if any
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    
{close_call}
}}

// Test: {method_name} initializes object properly  
TEST_F({class_name}_{method_name}_Test, InitializesObjectProperly) {{
{obj_creation}
{decls}
    bool result = obj.{method_name}({params});
    EXPECT_TRUE(result);
    
    // Wait for initialization
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    
{close_call}
}}
"""
        elif method_name == 'close':
            init_call = "    obj.init();\n    std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    " if has_init else ""
            content = f"""
// Test: {method_name} cleanup succeeds
TEST_F({class_name}_{method_name}_Test, CleanupSucceeds) {{
{obj_creation}
{init_call}
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
    }});
    
    // Wait for cleanup
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
}}

// Test: {method_name} handles repeated calls
TEST_F({class_name}_{method_name}_Test, HandlesRepeatedCalls) {{
{obj_creation}
{init_call}
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
        obj.{method_name}({params});
    }});
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
}}
"""
        else:
            init_call = "    obj.init();\n    std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    " if has_init else ""
            close_call = "    obj.close();\n    std::this_thread::sleep_for(std::chrono::milliseconds(100));" if has_close else ""
            content = f"""
// Test: {method_name} returns true on success
TEST_F({class_name}_{method_name}_Test, ReturnsTrueOnSuccess) {{
{obj_creation}
{init_call}
{decls}
    bool result = obj.{method_name}({params});
    EXPECT_TRUE(result);
    
{close_call}
}}

// Test: {method_name} handles multiple calls
TEST_F({class_name}_{method_name}_Test, HandlesMultipleCalls) {{
{obj_creation}
{init_call}
{decls}
    bool result1 = obj.{method_name}({params});
    bool result2 = obj.{method_name}({params});
    EXPECT_TRUE(result1 || result2);
    
{close_call}
}}

// Test: {method_name} state consistency
TEST_F({class_name}_{method_name}_Test, StateConsistency) {{
{obj_creation}
{init_call}
{decls}
    // Call multiple times and verify consistent behavior
    for (int i = 0; i < 3; i++) {{
        EXPECT_NO_THROW({{
            obj.{method_name}({params});
        }});
    }}
    
{close_call}
}}
"""
        return content
    
    def _generate_numeric_tests(self, class_name: str, method_name: str, method: Dict, class_info: Dict) -> str:
        """Generate tests for numeric return methods with threading safety"""
        
        # ALWAYS generate Python-based tests first
        decls, params = self._generate_param_values(method)
        
        # Get object creation code
        _, obj_deps, obj_decl = self._generate_object_creation_code(class_name, class_info)
        obj_creation = '\n'.join(obj_deps) + '\n' + obj_decl if obj_deps else obj_decl
        
        # Check if class has init/close methods
        has_init = self._has_init_method(class_info)
        has_close = self._has_close_method(class_info)
        
        init_call = "    obj.init();\n    std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    " if has_init else ""
        close_call = "    obj.close();\n    std::this_thread::sleep_for(std::chrono::milliseconds(100));" if has_close else ""
        
        # Methods like getTxStats, getRxStats need initialization first
        if 'Stats' in method_name or 'get' in method_name.lower():
            content = f"""
// Test: {method_name} returns valid value after initialization
TEST_F({class_name}_{method_name}_Test, ReturnsValidValueAfterInit) {{
{obj_creation}
{init_call}
{decls}
    auto result = obj.{method_name}({params});
    EXPECT_GE(result, 0); // Expect non-negative
    
{close_call}
}}

// Test: {method_name} returns zero initially
TEST_F({class_name}_{method_name}_Test, ReturnsZeroInitially) {{
{obj_creation}
{init_call}
{decls}
    auto result = obj.{method_name}({params});
    EXPECT_GE(result, 0); // Expect non-negative initial value
    
{close_call}
}}

// Test: {method_name} handles boundary values
TEST_F({class_name}_{method_name}_Test, HandlesBoundaryValues) {{
{obj_creation}
{init_call}
{decls}
    auto result = obj.{method_name}({params});
    EXPECT_GE(result, INT_MIN);
    EXPECT_LE(result, INT_MAX);
    
{close_call}
}}

// Test: {method_name} consistent across multiple calls
TEST_F({class_name}_{method_name}_Test, ConsistentAcrossMultipleCalls) {{
{obj_creation}
{init_call}
{decls}
    auto result1 = obj.{method_name}({params});
    auto result2 = obj.{method_name}({params});
    // Results should be valid
    EXPECT_GE(result1, 0);
    EXPECT_GE(result2, 0);
    
{close_call}
}}

// Test: {method_name} rapid sequential calls
TEST_F({class_name}_{method_name}_Test, RapidSequentialCalls) {{
{obj_creation}
{init_call}
{decls}
    // Test rapid sequential calls don't cause issues
    for (int i = 0; i < 10; i++) {{
        auto result = obj.{method_name}({params});
        EXPECT_GE(result, 0);
    }}
    
{close_call}
}}
"""
        else:
            content = f"""
// Test: {method_name} returns valid value
TEST_F({class_name}_{method_name}_Test, ReturnsValidValue) {{
{obj_creation}
{decls}
    auto result = obj.{method_name}({params});
    EXPECT_GE(result, 0); // Expect non-negative
}}

// Test: {method_name} handles boundary values
TEST_F({class_name}_{method_name}_Test, HandlesBoundaryValues) {{
{obj_creation}
{decls}
    auto result = obj.{method_name}({params});
    EXPECT_GE(result, INT_MIN);
    EXPECT_LE(result, INT_MAX);
}}

// Test: {method_name} non-negative return
TEST_F({class_name}_{method_name}_Test, NonNegativeReturn) {{
{obj_creation}
{decls}
    auto result = obj.{method_name}({params});
    EXPECT_GE(result, 0);
}}

// Test: {method_name} deterministic behavior
TEST_F({class_name}_{method_name}_Test, DeterministicBehavior) {{
{obj_creation}
{decls}
    // Multiple calls should not throw
    EXPECT_NO_THROW({{
        for (int i = 0; i < 5; i++) {{
            obj.{method_name}({params});
        }}
    }});
}}
"""
        return content
    
    def _generate_void_tests(self, class_name: str, method_name: str, method: Dict, class_info: Dict) -> str:
        """Generate tests for void methods with improved logic and threading safety"""
        
        # ALWAYS generate Python-based tests first
        decls, params = self._generate_param_values(method)
        
        # Get object creation code
        _, obj_deps, obj_decl = self._generate_object_creation_code(class_name, class_info)
        obj_creation = '\n'.join(obj_deps) + '\n' + obj_decl if obj_deps else obj_decl
        
        # Check if class has init/close methods
        has_init = self._has_init_method(class_info)
        has_close = self._has_close_method(class_info)
        
        # Special handling for methods that might involve threading
        if method_name in ['addToTx', 'addToRx', 'addToQueue']:
            content = f"""
// Test: {method_name} executes successfully after initialization
TEST_F({class_name}_{method_name}_Test, ExecutesSuccessfullyAfterInit) {{
    {class_name} obj;
    // Initialize first
    obj.init();
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
    }});
    
    // Cleanup
    obj.close();
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
}}

// Test: {method_name} can be called multiple times
TEST_F({class_name}_{method_name}_Test, MultipleCallsSafe) {{
    {class_name} obj;
    obj.init();
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
        obj.{method_name}({params});
        obj.{method_name}({params});
    }});
    
    obj.close();
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
}}
"""
        elif method_name == 'init':
            content = f"""
// Test: {method_name} executes successfully
TEST_F({class_name}_{method_name}_Test, ExecutesSuccessfully) {{
    {class_name} obj;
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
    }});
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    obj.close();
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
}}

// Test: {method_name} can be called multiple times safely
TEST_F({class_name}_{method_name}_Test, MultipleCallsSafe) {{
    {class_name} obj;
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        obj.close();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        obj.{method_name}({params});
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        obj.close();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }});
}}
"""
        elif method_name == 'close':
            content = f"""
// Test: {method_name} executes successfully
TEST_F({class_name}_{method_name}_Test, ExecutesSuccessfully) {{
    {class_name} obj;
    obj.init();
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
    }});
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
}}

// Test: {method_name} can be called multiple times safely
TEST_F({class_name}_{method_name}_Test, MultipleCallsSafe) {{
    {class_name} obj;
    obj.init();
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
        obj.{method_name}({params});
    }});
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
}}
"""
        else:
            # For other void methods (like 'run')
            init_call = "    obj.init();\n    std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    " if has_init else ""
            close_call = "    obj.close();\n    std::this_thread::sleep_for(std::chrono::milliseconds(100));" if has_close else ""
            
            content = f"""
// Test: {method_name} executes without throwing
TEST_F({class_name}_{method_name}_Test, ExecutesWithoutThrowing) {{
{obj_creation}
{init_call}
{decls}
    EXPECT_NO_THROW({{
        std::thread runThread([&obj]() {{ obj.{method_name}({params}); }});
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        runThread.detach();
    }});
    
{close_call}
}}

// Test: {method_name} can be called multiple times
TEST_F({class_name}_{method_name}_Test, CanBeCalledMultipleTimes) {{
{obj_creation}
{decls}
    EXPECT_NO_THROW({{
        std::thread runThread([&obj]() {{ obj.{method_name}({params}); }});
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        runThread.detach();
    }});
}}

// Test: {method_name} handles null/invalid conditions
TEST_F({class_name}_{method_name}_Test, HandlesInvalidConditions) {{
{obj_creation}
{decls}
    // Test with boundary/edge case parameters
    EXPECT_NO_THROW({{
        std::thread runThread([&obj]() {{ obj.{method_name}({params}); }});
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        runThread.detach();
    }});
}}
"""
        return content
    
    def _generate_generic_tests(self, class_name: str, method_name: str, method: Dict, class_info: Dict) -> str:
        """Generate generic tests for other return types"""
        
        # ALWAYS generate Python-based tests first
        decls, params = self._generate_param_values(method)
        
        # Get object creation code
        _, obj_deps, obj_decl = self._generate_object_creation_code(class_name, class_info)
        obj_creation = '\n'.join(obj_deps) + '\n' + obj_decl if obj_deps else obj_decl
        
        content = f"""
// Test: {method_name} returns valid result
TEST_F({class_name}_{method_name}_Test, ReturnsValidResult) {{
{obj_creation}
{decls}
    EXPECT_NO_THROW({{
        auto result = obj.{method_name}({params});
        // Result should be valid
    }});
}}

// Test: {method_name} handles edge cases
TEST_F({class_name}_{method_name}_Test, HandlesEdgeCases) {{
{obj_creation}
{decls}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
    }});
}}
"""
        return content
    
    def _generate_param_values(self, method: Dict) -> Tuple[str, str]:
        """Generate parameter values for method calls
        Returns: (variable_declarations, param_values)
        """
        if not method['parameters']:
            return ("", "")
        
        declarations = []
        values = []
        var_counter = 0
        
        for param in method['parameters']:
            param_type = param['type']
            if 'int' in param_type:
                values.append('0')
            elif 'bool' in param_type:
                values.append('true')
            elif 'string' in param_type:
                values.append('""')
            elif '&' in param_type:
                # Reference type - need to create a variable
                base_type = param_type.replace('&', '').replace('const', '').strip()
                var_name = f'param_{var_counter}'
                declarations.append(f'        {base_type} {var_name};')
                values.append(var_name)
                var_counter += 1
            elif '*' in param_type:
                # Pointer type - distinguish between input and output parameters
                if 'char' in param_type:
                    # const char* or char* - use empty string or explicit cast
                    if 'const' in param_type:
                        values.append('""')  # Empty string literal for const char*
                    else:
                        values.append('static_cast<char*>(nullptr)')
                elif 'FILE' in param_type:
                    # FILE* - use explicit cast to avoid ambiguity
                    values.append('static_cast<FILE*>(nullptr)')
                else:
                    # Other pointer types - likely output parameters, create a variable
                    base_type = param_type.replace('*', '').replace('const', '').strip()
                    var_name = f'param_{var_counter}'
                    
                    # Create a variable of the base type and pass its address
                    if 'bool' in base_type:
                        declarations.append(f'        {base_type} {var_name} = false;')
                    elif any(t in base_type for t in ['int', 'long', 'short', 'size_t', 'uint', 'int64', 'unsigned']):
                        declarations.append(f'        {base_type} {var_name} = 0;')
                    elif any(t in base_type for t in ['float', 'double']):
                        declarations.append(f'        {base_type} {var_name} = 0.0;')
                    else:
                        declarations.append(f'        {base_type} {var_name};')
                    
                    values.append(f'&{var_name}')
                    var_counter += 1
            else:
                values.append(f'{param_type}()')
        
        decl_str = '\n'.join(declarations) if declarations else ""
        return (decl_str, ', '.join(values))
    
    def _generate_param_boundary_values(self, method: Dict) -> List[Tuple[str, str, str]]:
        """Generate boundary value test cases for method parameters
        Returns: List of (test_description, declarations, param_values) tuples
        """
        if not method['parameters']:
            return [("Default", "", "")]
        
        test_cases = []
        
        # Generate default case
        decls, params = self._generate_param_values(method)
        test_cases.append(("Default", decls, params))
        
        # Generate boundary cases for each parameter type
        for idx, param in enumerate(method['parameters']):
            param_type = param['type']
            
            if 'int' in param_type and 'unsigned' not in param_type:
                # Negative value
                test_params = []
                for i, p in enumerate(method['parameters']):
                    if i == idx:
                        test_params.append('-1')
                    elif 'int' in p['type']:
                        test_params.append('0')
                    elif 'bool' in p['type']:
                        test_params.append('true')
                    elif 'string' in p['type']:
                        test_params.append('""')
                    else:
                        test_params.append(f"{p['type']}()")
                test_cases.append((f"NegativeParam{idx}", "", ', '.join(test_params)))
                
                # Max value
                test_params = []
                for i, p in enumerate(method['parameters']):
                    if i == idx:
                        test_params.append('INT_MAX')
                    elif 'int' in p['type']:
                        test_params.append('0')
                    elif 'bool' in p['type']:
                        test_params.append('true')
                    elif 'string' in p['type']:
                        test_params.append('""')
                    else:
                        test_params.append(f"{p['type']}()")
                test_cases.append((f"MaxParam{idx}", "", ', '.join(test_params)))
            
            elif 'bool' in param_type:
                # False value
                test_params = []
                for i, p in enumerate(method['parameters']):
                    if i == idx:
                        test_params.append('false')
                    elif 'int' in p['type']:
                        test_params.append('0')
                    elif 'bool' in p['type']:
                        test_params.append('true')
                    elif 'string' in p['type']:
                        test_params.append('""')
                    else:
                        test_params.append(f"{p['type']}()")
                test_cases.append((f"FalseParam{idx}", "", ', '.join(test_params)))
        
        return test_cases
    
    def _generate_exception_safety_tests(self, class_name: str, method_name: str, method: Dict, class_info: Dict) -> str:
        """Generate exception safety and robustness tests"""
        decls, params = self._generate_param_values(method)
        _, obj_deps, obj_decl = self._generate_object_creation_code(class_name, class_info)
        obj_creation = '\n'.join(obj_deps) + '\n' + obj_decl if obj_deps else obj_decl
        
        # Check if class has init/close methods
        has_init = self._has_init_method(class_info)
        has_close = self._has_close_method(class_info)
        init_call = "    obj.init();\n    std::this_thread::sleep_for(std::chrono::milliseconds(50));\n    " if has_init else ""
        close_call = "    obj.close();\n    std::this_thread::sleep_for(std::chrono::milliseconds(50));" if has_close else ""
        
        content = f"""
// Test: {method_name} exception safety
TEST_F({class_name}_{method_name}_Test, ExceptionSafety) {{
{obj_creation}
{init_call}
{decls}
    // Method should not throw unexpected exceptions
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
    }});
{close_call}
}}

// Test: {method_name} resource cleanup on failure
TEST_F({class_name}_{method_name}_Test, ResourceCleanupOnFailure) {{
{obj_creation}
{init_call}
{decls}
    // Even if method fails, object should remain in valid state
    try {{
        obj.{method_name}({params});
    }} catch (...) {{
        // Object should still be destructible
    }}
{close_call}
}}
"""
        return content
    
    def _generate_stress_tests(self, class_name: str, method_name: str, method: Dict, class_info: Dict) -> str:
        """Generate stress and performance tests"""
        decls, params = self._generate_param_values(method)
        _, obj_deps, obj_decl = self._generate_object_creation_code(class_name, class_info)
        obj_creation = '\n'.join(obj_deps) + '\n' + obj_decl if obj_deps else obj_decl
        
        has_init = self._has_init_method(class_info)
        has_close = self._has_close_method(class_info)
        init_call = "    obj.init();\n    std::this_thread::sleep_for(std::chrono::milliseconds(50));\n    " if has_init else ""
        close_call = "    obj.close();\n    std::this_thread::sleep_for(std::chrono::milliseconds(50));" if has_close else ""
        
        content = f"""
// Test: {method_name} stress test
TEST_F({class_name}_{method_name}_Test, StressTest) {{
{obj_creation}
{init_call}
{decls}
    // Call method many times rapidly
    for (int i = 0; i < 100; i++) {{
        EXPECT_NO_THROW({{
            obj.{method_name}({params});
        }});
    }}
{close_call}
}}
"""
        return content
    
    def write_test_file(self, source_file: Path, class_info: Dict, method: Dict, 
                        dependent_headers: Set[str]):
        """Write test file for a method"""
        if not class_info or not method:
            return
        
        class_name = class_info['class_name']
        method_name = method['name']
        is_static = method.get('is_static', False)
        
        # Skip abstract classes (has pure virtual methods) unless the method is static
        if class_info.get('is_abstract', False) and not is_static:
            return  # Can't instantiate abstract classes
        
        # Skip nested/inner classes (not supported)
        if '::' in class_name:
            return
        
        # For static methods, we don't need to instantiate the class
        if not is_static:
            # Check if the class has a default constructor or can be instantiated easily
            constructor_info = self._get_constructor_info(class_info)
            
            # If class only has parameterized constructors and no default, check complexity
            if constructor_info['has_params'] and not constructor_info['has_default']:
                # Can't easily instantiate classes that require parameters and have no default constructor
                return
            
            # If class has parameters, check if they're complex
            if constructor_info['has_params']:
                for param in constructor_info['parameters']:
                    param_type = param['type'].replace('&', '').replace('const', '').replace('*', '').strip()
                    # Skip if parameter type contains &&  (rvalue reference) or is complex
                    if '&&' in param['type'] or 'Builder' in param_type:
                        return
        
        # NEW STRATEGY: Generate micro-tests - separate test file for each test case
        # This improves granularity and makes individual tests easier to debug
        self.write_micro_tests(source_file, class_info, method, dependent_headers)
    
    def write_micro_tests(self, source_file: Path, class_info: Dict, method: Dict,
                          dependent_headers: Set[str]):
        """Generate multiple micro-test files for a single method - one file per test case"""
        class_name = class_info['class_name']
        method_name = method['name']
        source_name = source_file.stem
        
        # Skip destructors - they can't be tested this way
        if method.get('is_destructor', False):
            return
        
        # Generate different test scenarios
        test_scenarios = []
        
        is_static = method.get('is_static', False)
        
        # For static methods, generate simpler test scenarios
        if is_static:
            if method['is_constructor']:
                return  # Skip constructors marked as static (shouldn't happen)
            elif 'bool' in method['return_type']:
                test_scenarios.extend([
                    ('ReturnValue', 'Test static method returns value'),
                    ('NoThrow', 'Test static method executes without throwing'),
                    ('MultipleInvocations', 'Test static method handles multiple calls'),
                    ('ConsistentResults', 'Test static method returns consistent results')
                ])
            elif 'void' in method['return_type']:
                test_scenarios.extend([
                    ('NoThrow', 'Test static method executes without throwing'),
                    ('MultipleInvocations', 'Test static method handles multiple calls'),
                    ('RapidCalls', 'Test static method handles rapid successive calls')
                ])
            elif any(t in method['return_type'] for t in ['int', 'long', 'size_t', 'unsigned', 'char', 'double', 'float']):
                test_scenarios.extend([
                    ('ValidReturn', 'Test static method returns valid value'),
                    ('NoThrow', 'Test static method executes without throwing'),
                    ('MultipleInvocations', 'Test static method handles multiple calls'),
                    ('ConsistentResults', 'Test static method returns consistent results')
                ])
            else:
                test_scenarios.extend([
                    ('ValidReturn', 'Test static method returns valid result'),
                    ('NoThrow', 'Test static method executes without throwing'),
                    ('MultipleInvocations', 'Test static method handles multiple calls')
                ])
        elif method['is_constructor']:
            test_scenarios.extend([
                ('BasicConstruction', 'Test that object can be constructed'),
                ('MultipleInstances', 'Test that multiple objects can be constructed')
            ])
        elif 'bool' in method['return_type']:
            test_scenarios.extend([
                ('ReturnTrue', 'Test method returns true in success case'),
                ('ReturnFalse', 'Test method returns false in failure case'),
                ('MultipleInvocations', 'Test method handles multiple calls'),
                ('ConsistentBehavior', 'Test method behavior is consistent')
            ])
        elif 'void' in method['return_type']:
            test_scenarios.extend([
                ('NoThrow', 'Test method executes without throwing'),
                ('MultipleInvocations', 'Test method can be called multiple times'),
                ('RapidCalls', 'Test method handles rapid successive calls')
            ])
        elif any(t in method['return_type'] for t in ['int', 'long', 'size_t', 'unsigned']):
            test_scenarios.extend([
                ('ValidReturn', 'Test method returns valid value'),
                ('BoundaryCheck', 'Test return value within expected range'),
                ('Consistency', 'Test method returns consistent values'),
                ('MultipleInvocations', 'Test method can be called multiple times')
            ])
        else:
            test_scenarios.extend([
                ('ValidReturn', 'Test method returns valid result'),
                ('NoThrow', 'Test method executes without throwing')
            ])
        
        # Generate a separate test file for each scenario
        for scenario_name, scenario_desc in test_scenarios:
            self._write_single_micro_test(source_file, class_info, method, dependent_headers,
                                          scenario_name, scenario_desc)
    
    def _write_single_micro_test(self, source_file: Path, class_info: Dict, method: Dict,
                                  dependent_headers: Set[str], scenario_name: str, scenario_desc: str):
        """Write a single micro-test file for one specific test scenario"""
        class_name = class_info['class_name']
        method_name = method['name']
        source_name = source_file.stem
        
        # Generate filename: <filename>_<method>_<scenario>.cpp
        test_filename = f"{source_name}_{method_name}_{scenario_name}.cpp"
        
        # Generate test content for this specific scenario
        test_content = self._generate_micro_test_content(source_file, class_info, method,
                                                         dependent_headers, scenario_name, scenario_desc)
        
        if not test_content:
            return
        
        # STEP 1: Save Python-generated version to backup folder (if Ollama enabled)
        if self.use_ollama:
            python_backup_path = self.python_test_dir / test_filename
            with open(python_backup_path, 'w') as f:
                f.write(test_content)
        
        # STEP 2: If Ollama is enabled, try to enhance the test
        enhanced_content = None
        if self.use_ollama:
            has_init = self._has_init_method(class_info) or method_name == 'init'
            has_close = self._has_close_method(class_info) or method_name == 'close'
            
            # Try to enhance with Ollama
            enhanced_content = self._enhance_test_with_ollama_full(
                test_content, class_name, method_name, method, has_init, has_close
            )
        
        # STEP 3: Write the final version (enhanced if available, otherwise Python)
        output_path = self.output_dir / test_filename
        final_content = enhanced_content if enhanced_content else test_content
        with open(output_path, 'w') as f:
            f.write(final_content)
        
        # Print with appropriate indicator
        if enhanced_content and self.use_ollama:
            print(f"  Generated micro-test: {test_filename} (ðŸ¤– Ollama-enhanced)")
        elif self.use_ollama and not enhanced_content:
            print(f"  Generated micro-test: {test_filename} (ðŸ“ Python fallback)")
        else:
            print(f"  Generated micro-test: {test_filename}")
        
        # Store metadata
        self.test_metadata.append({
            'test_file': str(output_path),
            'python_backup': str(python_backup_path) if self.use_ollama else None,
            'source_file': str(source_file),
            'test_name': output_path.stem,
            'class_name': class_info['class_name'],
            'method_name': method_name,
            'header_file': class_info['header_file'],
            'scenario': scenario_name,
            'ollama_enhanced': bool(enhanced_content and self.use_ollama)
        })
    
    def _generate_micro_test_content(self, source_file: Path, class_info: Dict, method: Dict,
                                      dependent_headers: Set[str], scenario_name: str, scenario_desc: str) -> str:
        """Generate content for a single micro-test"""
        class_name = class_info['class_name']
        method_name = method['name']
        
        # Determine the correct include path
        header_file = class_info['header_file']
        header_full_path = Path(class_info.get('header_full_path', ''))
        
        if 'catch' in header_file.lower() and header_full_path.exists():
            src_dir = self.source_root / 'src'
            try:
                rel_path = header_full_path.relative_to(src_dir)
                include_path = f"<{rel_path.as_posix()}>"
            except ValueError:
                include_path = f"<catch2/{header_file}>"
        else:
            include_path = f'"{header_file}"'
        
        content = f"""// Micro-test for {class_name}::{method_name} - {scenario_desc}
#include <gtest/gtest.h>
#include <climits>

// Include actual header being tested
#include {include_path}

"""
        
        # Add namespace declarations
        namespace = class_info.get('namespace', '')
        if namespace:
            # Add using namespace declaration for the detected namespace
            content += f"using namespace {namespace};\n\n"
        elif 'catch' in class_info['header_file'].lower():
            # Fallback for Catch2 if namespace wasn't detected
            content += "using namespace Catch;\n"
            if 'textflow' in class_info['header_file'].lower():
                content += "using namespace Catch::TextFlow;\n"
            content += "\n"
        
        # Check if this is a static method
        is_static = method.get('is_static', False)
        
        # Skip test generation for classes with protected destructors (cannot be instantiated) unless static
        if class_info.get('has_protected_destructor', False) and not is_static:
            return None
        
        # For static methods, we don't need object creation
        if is_static:
            obj_creation = ""
            decls, params = self._generate_param_values(method)
            param_setup = decls if decls else ""
            
            # Generate test for static method based on scenario
            if scenario_name == 'ValidReturn':
                content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{param_setup}
    auto result = {class_name}::{method_name}({params});
    // Verify result is accessible
    (void)result; // Mark as used
    SUCCEED();
}}
"""
            elif scenario_name == 'NoThrow':
                content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{param_setup}
    EXPECT_NO_THROW({{
        {class_name}::{method_name}({params});
    }});
}}
"""
            else:
                content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{param_setup}
    EXPECT_NO_THROW({{
        {class_name}::{method_name}({params});
    }});
}}
"""
            return content
        
        # Generate single focused test for non-static methods
        _, obj_deps, obj_decl = self._generate_object_creation_code(class_name, class_info)
        
        # Skip test generation if object cannot be created (no suitable constructor)
        if obj_decl is None:
            return None
        
        obj_creation = '\n'.join(obj_deps) + '\n' + obj_decl if obj_deps else obj_decl
        
        decls, params = self._generate_param_values(method)
        param_setup = decls if decls else ""
        
        # Generate test based on scenario
        if scenario_name == 'BasicConstruction':
            content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{obj_creation}
    SUCCEED(); // Object constructed successfully
}}
"""
        elif scenario_name == 'NoThrow':
            content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{obj_creation}
{param_setup}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
    }});
}}
"""
        elif scenario_name == 'ReturnTrue':
            content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{obj_creation}
{param_setup}
    bool result = obj.{method_name}({params});
    EXPECT_TRUE(result);
}}
"""
        elif scenario_name == 'ValidReturn':
            content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{obj_creation}
{param_setup}
    auto result = obj.{method_name}({params});
    // Verify result is accessible
    (void)result; // Mark as used
    SUCCEED();
}}
"""
        elif scenario_name == 'MultipleInvocations':
            content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{obj_creation}
{param_setup}
    // Test can be called 3 times without issues
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
        obj.{method_name}({params});
        obj.{method_name}({params});
    }});
}}
"""
        elif scenario_name == 'BoundaryCheck':
            content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{obj_creation}
{param_setup}
    auto result = obj.{method_name}({params});
    EXPECT_GE(result, 0); // At minimum, expect non-negative
}}
"""
        else:
            # Default simple test
            content += f"""TEST({class_name}_{method_name}Test, {scenario_name}) {{
{obj_creation}
{param_setup}
    EXPECT_NO_THROW({{
        obj.{method_name}({params});
    }});
}}
"""
        
        return content
        
        # Generate filename: <filename>_<method>.cpp
        source_name = source_file.stem  # e.g., "Program"
        method_name = method['name']
        test_filename = f"{source_name}_{method_name}.cpp"
        class_name = class_info['class_name']
        
        # STEP 1: Save Python-generated version to backup folder
        python_backup_path = self.python_test_dir / test_filename
        with open(python_backup_path, 'w') as f:
            f.write(test_content)
        
        # STEP 2: If Ollama is enabled, plan and enhance the test
        enhanced_content = None
        if self.use_ollama:
            has_init = self._has_init_method(class_info) or method_name == 'init'
            has_close = self._has_close_method(class_info) or method_name == 'close'
            
            # Add to enhancement plan
            plan = self._plan_enhancement(class_name, method_name, method, has_init, has_close)
            self.enhancement_plan.append(plan)
            
            # Try to enhance with Ollama
            enhanced_content = self._enhance_test_with_ollama_full(
                test_content, class_name, method_name, method, has_init, has_close
            )
        
        # STEP 3: Write the final version (enhanced if available, otherwise Python)
        output_path = self.output_dir / test_filename
        final_content = enhanced_content if enhanced_content else test_content
        with open(output_path, 'w') as f:
            f.write(final_content)
        
        if enhanced_content and self.use_ollama:
            print(f"  Generated test: {test_filename} (ðŸ¤– Ollama-enhanced)")
        else:
            print(f"  Generated test: {test_filename}")
        
        # Store metadata for building
        self.test_metadata.append({
            'test_file': str(output_path),
            'python_backup': str(python_backup_path),
            'source_file': str(source_file),
            'test_name': output_path.stem,
            'class_name': class_info['class_name'],
            'method_name': method_name,
            'header_file': class_info['header_file'],
            'ollama_enhanced': bool(enhanced_content and self.use_ollama)
        })
    
    def save_metadata(self):
        """Save all test metadata to JSON file"""
        metadata_path = self.output_dir.parent / "test_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(self.test_metadata, f, indent=2)
        print(f"\n  Saved metadata: {metadata_path}")


class TestBuilder:
    """Builds and runs tests using g++ directly"""
    
    def __init__(self, output_root: Path, mock_dir: Path, source_root: Path):
        self.output_root = output_root
        self.mock_dir = mock_dir
        # Convert source_root to absolute path to ensure file operations work correctly
        self.source_root = source_root.resolve() if isinstance(source_root, Path) else Path(source_root).resolve()
        self.build_dir = output_root / "bin"
        self.build_dir.mkdir(parents=True, exist_ok=True)
        
        # GoogleTest paths
        self.gtest_root = Path("/workspaces/CppMicroAgent/googletest-1.16.0")
        self.gtest_include = self.gtest_root / "googletest" / "include"
        self.gtest_lib_dir = self.gtest_root / "build" / "lib"
        
        # Detect project structure type
        self.is_header_only = self._detect_header_only_library()
        
        # Find all source subdirectories
        self.include_dirs = [
            str(self.gtest_include),  # GoogleTest headers
            str(self.source_root),     # Project root (for headers in root directory) - now absolute
        ]
        
        # For header-only libraries, add the source root first to support <project/header.h> includes
        if self.is_header_only:
            # Add the src directory to support includes like <catch2/...>
            # This allows #include <catch2/file.hpp> when catch2 is a subdir of src
            if (self.source_root / 'src').exists():
                self.include_dirs.append(str(self.source_root / 'src'))
        
        # Add inc directory if it exists
        if (self.source_root / 'inc').exists():
            self.include_dirs.append(str(self.source_root / 'inc'))
        
        # Add all subdirectories in src BEFORE mocks so real headers are found first
        if (self.source_root / 'src').exists():
            for subdir in (self.source_root / 'src').rglob('*'):
                if subdir.is_dir():
                    self.include_dirs.append(str(subdir))
        
        # Add mock directory LAST so it's only used for missing headers
        self.include_dirs.append(str(self.mock_dir))
        
        # Find all source files for linking 
        # For header-only libraries like Catch2, we'll build a static library
        self.all_source_files = []
        self.project_lib = None  # Path to static library if built
        
        # First check for .cpp files in project root (like tinyxml2)
        for cpp_file in self.source_root.glob('*.cpp'):
            filename = cpp_file.name.lower()
            # Exclude main files and obvious test files (but not library files like tinyxml2.cpp)
            if 'main' in filename and filename.startswith('main'):
                continue
            if filename.startswith('test') or filename.endswith('test.cpp') or filename.endswith('tests.cpp'):
                continue
            self.all_source_files.append(str(cpp_file))
        
        # Then check src directory
        if (self.source_root / 'src').exists():
            for cpp_file in (self.source_root / 'src').rglob('*.cpp'):
                filename = cpp_file.name.lower()
                # Exclude main files from the library to avoid conflicts with GoogleTest's main
                if 'main' in filename or filename.startswith('test') or 'catch_main' in filename:
                    continue
                self.all_source_files.append(str(cpp_file))
        
        # If this is a header-only library with many source files, build a static library
        if self.is_header_only and len(self.all_source_files) > 20:
            print(f"  ðŸ“¦ Building static library from {len(self.all_source_files)} source files...")
            self.project_lib = self._build_static_library()
            if self.project_lib:
                print(f"  âœ… Static library built: {self.project_lib}")
            else:
                print(f"  âš ï¸  Failed to build static library, will link source files individually")
        
        # Tests that are known to be problematic (high-level interfaces with threading)
        self.skip_run_patterns = [
            'InterfaceA_', 'InterfaceB_',  # High-level interfaces with threading
            'ProgramApp_', 'Program_'       # Program classes that start threads
        ]
    
    def _detect_header_only_library(self) -> bool:
        """Detect if this is a header-only library by checking the ratio of headers to cpp files"""
        src_dir = self.source_root / 'src'
        if not src_dir.exists():
            return False
        
        cpp_files = list(src_dir.rglob('*.cpp'))
        header_files = list(src_dir.rglob('*.h')) + list(src_dir.rglob('*.hpp'))
        
        # If there are many more headers than cpp files, likely header-only or mostly header-only
        if len(header_files) > 0 and len(cpp_files) > 0:
            ratio = len(header_files) / len(cpp_files)
            # If ratio > 1.5, consider it header-only oriented
            if ratio > 1.5:
                print(f"  ðŸ“š Detected header-only library (headers: {len(header_files)}, cpp: {len(cpp_files)})")
                return True
        
        return False
    
    def _build_static_library(self) -> Path:
        """Build a static library from all source files to speed up linking"""
        lib_path = self.build_dir / "libproject.a"
        
        # Skip if library already exists and is recent
        if lib_path.exists():
            print(f"  â„¹ï¸  Using existing static library")
            return lib_path
        
        print(f"  ðŸ”¨ Compiling {len(self.all_source_files)} source files...")
        
        # Compile all source files to object files
        object_files = []
        
        for i, cpp_file in enumerate(self.all_source_files):
            if i % 20 == 0 and i > 0:
                print(f"  ... compiled {i}/{len(self.all_source_files)}")
            
            cpp_path = Path(cpp_file)
            obj_file = self.build_dir / (cpp_path.stem + '.o')
            
            # Compile to object file
            cmd = [
                'g++',
                '-std=c++14',
                '-c',  # Compile only, don't link
                '-o', str(obj_file),
                str(cpp_file)
            ]
            
            # Add include directories
            for inc_dir in self.include_dirs:
                cmd.extend(['-I', inc_dir])
            
            try:
                result = subprocess.run(cmd, capture_output=True, timeout=30)
                if result.returncode == 0:
                    object_files.append(str(obj_file))
            except:
                pass
        
        if len(object_files) < len(self.all_source_files) * 0.5:
            # If less than half compiled, something is wrong
            print(f"  âš ï¸  Only {len(object_files)}/{len(self.all_source_files)} files compiled")
            return None
        
        print(f"  âœ… Compiled {len(object_files)} object files")
        print(f"  ðŸ”— Creating static library...")
        
        # Create static library using ar
        cmd = ['ar', 'rcs', str(lib_path)] + object_files
        
        try:
            result = subprocess.run(cmd, capture_output=True, timeout=60)
            if result.returncode == 0:
                return lib_path
        except:
            pass
        
        return None
    
    def compile_test(self, test_metadata: Dict) -> bool:
        """Compile a single test using g++, with fallback to Python version if Ollama-enhanced fails"""
        test_file = test_metadata['test_file']
        source_file = test_metadata['source_file']
        test_name = test_metadata['test_name']
        ollama_enhanced = test_metadata.get('ollama_enhanced', False)
        python_backup = test_metadata.get('python_backup', None)
        
        output_binary = self.build_dir / test_name
        
        def try_compile(file_path, label=""):
            """Helper function to try compiling a test file"""
            # Build g++ command with --coverage flag for gcda/gcno generation
            cmd = [
                'g++',
                '-std=c++14',  # GoogleTest 1.16.0 requires C++14
                '--coverage',  # Enable coverage instrumentation (equivalent to -fprofile-arcs -ftest-coverage)
                '-o', str(output_binary),
                file_path,
            ]
            
            # Add source files for linking
            if self.project_lib and self.project_lib.exists():
                # Use the static library if available
                cmd.append(str(self.project_lib))
            elif self.is_header_only:
                # For header-only libraries like Catch2, we need to link ALL implementation files
                # because they have interdependencies (e.g., catch_approx.cpp needs ReusableStringStream)
                cmd.extend(self.all_source_files)
            else:
                # For regular projects, link all source files
                cmd.extend(self.all_source_files)
            
            # Add include directories
            for inc_dir in self.include_dirs:
                cmd.extend(['-I', inc_dir])
            
            # Add library directory and libraries
            cmd.extend([
                '-L', str(self.gtest_lib_dir),
                '-lgtest',
                '-lgtest_main',
                '-lpthread',
                '-lgcov',  # Link with gcov library for coverage
            ])
            
            try:
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                
                return result.returncode == 0, result.stderr
            except subprocess.TimeoutExpired:
                return False, "Timeout"
            except Exception as e:
                return False, str(e)
        
        print(f"  Compiling {test_name}...", end=' ')
        
        # Try compiling the current version (might be Ollama-enhanced or Python)
        success, error = try_compile(test_file)
        
        if success:
            if ollama_enhanced:
                print("âœ… SUCCESS (Ollama-enhanced)")
            else:
                print("âœ… SUCCESS")
            return True
        
        # If compilation failed and we have an Ollama-enhanced version, try Python backup
        if ollama_enhanced and python_backup and Path(python_backup).exists():
            print("âŒ FAILED")
            print(f"    ðŸ”„ Ollama-enhanced version failed compilation")
            print(f"    ðŸ“ Trying Python-generated fallback...")
            
            # Copy Python backup to replace the failed version
            import shutil
            shutil.copy(python_backup, test_file)
            
            # Try compiling Python version
            print(f"  Compiling {test_name} (Python fallback)...", end=' ')
            success, error = try_compile(test_file)
            
            if success:
                print("âœ… SUCCESS (Python fallback)")
                # Update metadata to reflect that we're using Python version
                test_metadata['ollama_enhanced'] = False
                test_metadata['fallback_used'] = True
                return True
            else:
                print("âŒ FAILED (both versions)")
                print(f"    Error: {error[:200]}")
                return False
        else:
            # No backup or not Ollama-enhanced, just report failure
            print("âŒ FAILED")
            print(f"    Error: {error[:200]}")
            return False
    
    def run_test(self, test_metadata: Dict) -> tuple:
        """Run a compiled test
        Returns: (passed: bool, skipped: bool)
        """
        test_name = test_metadata['test_name']
        binary = self.build_dir / test_name
        
        if not binary.exists():
            print(f"  âš ï¸  Binary not found: {test_name}")
            return (False, False)
        
        # Check if this test should be skipped due to known threading issues
        should_skip = any(pattern in test_name for pattern in self.skip_run_patterns)
        if should_skip:
            print(f"  â­ï¸  Skipped {test_name} (known threading issues)")
            return (False, True)
        
        print(f"  Running {test_name}...", end=' ')
        
        try:
            result = subprocess.run(
                ['./' + test_name],  # Run with relative path since we're in the build directory
                capture_output=True,
                text=True,
                timeout=10,
                cwd=str(self.build_dir)  # Run from bin directory so .gcda files are created in the right place
            )
            
            if result.returncode == 0:
                # Count passed tests
                passed = result.stdout.count('[  PASSED  ]')
                print(f"âœ… PASSED ({passed} tests)")
                return (True, False)
            else:
                print("âŒ FAILED")
                # Show first failure
                lines = result.stdout.split('\n')
                for line in lines:
                    if 'FAILED' in line or 'Failure' in line:
                        print(f"    {line}")
                        break
                return (False, False)
        except subprocess.TimeoutExpired:
            print("âŒ TIMEOUT")
            return (False, False)
        except Exception as e:
            print(f"âŒ ERROR: {e}")
            return (False, False)
    
    def build_and_run_all(self, metadata_file: Path):
        """Build and run all tests"""
        with open(metadata_file, 'r') as f:
            all_metadata = json.load(f)
        
        print(f"\n{'='*70}")
        print(f"BUILDING TESTS ({len(all_metadata)} tests)")
        print(f"{'='*70}\n")
        
        compiled = []
        failed_compile = []
        fallback_used = []
        
        for metadata in all_metadata:
            if self.compile_test(metadata):
                compiled.append(metadata)
                if metadata.get('fallback_used', False):
                    fallback_used.append(metadata)
            else:
                failed_compile.append(metadata)
        
        print(f"\n{'='*70}")
        print(f"RUNNING TESTS ({len(compiled)} compiled)")
        print(f"{'='*70}\n")
        
        passed = []
        failed_run = []
        skipped = []
        
        for metadata in compiled:
            success, skip = self.run_test(metadata)
            if skip:
                skipped.append(metadata)
            elif success:
                passed.append(metadata)
            else:
                failed_run.append(metadata)
        
        # Count Ollama enhancements
        ollama_enhanced = sum(1 for m in all_metadata if m.get('ollama_enhanced', False))
        ollama_successful = sum(1 for m in compiled if m.get('ollama_enhanced', False) and not m.get('fallback_used', False))
        
        # Print summary
        print(f"\n{'='*70}")
        print(f"TEST SUMMARY")
        print(f"{'='*70}")
        print(f"  Total Tests:      {len(all_metadata)}")
        print(f"  Compiled:         {len(compiled)} âœ…")
        print(f"  Failed Compile:   {len(failed_compile)} âŒ")
        print(f"  Passed:           {len(passed)} âœ…")
        print(f"  Failed Run:       {len(failed_run)} âŒ")
        print(f"  Skipped:          {len(skipped)} â­ï¸  (threading issues)")
        
        # Show Ollama stats if applicable
        if ollama_enhanced > 0:
            print(f"\n  ðŸ¤– Ollama Enhancement Stats:")
            print(f"     Total Enhanced:      {ollama_enhanced}")
            print(f"     Successfully Used:   {ollama_successful}")
            print(f"     Fallback to Python:  {len(fallback_used)}")
            if ollama_successful > 0:
                success_rate = (ollama_successful / ollama_enhanced) * 100
                print(f"     Enhancement Success: {success_rate:.1f}%")
        
        print(f"{'='*70}")
        
        # Calculate success rate for non-skipped tests
        runnable = len(compiled) - len(skipped)
        if runnable > 0:
            success_rate = (len(passed) / runnable) * 100
            print(f"\n  Success Rate (excl. skipped): {success_rate:.1f}% ({len(passed)}/{runnable})")
        
        if len(fallback_used) > 0:
            print(f"\nðŸ’¡ Note: {len(fallback_used)} Ollama-enhanced test(s) failed to compile and used Python backup.")
        print(f"\nðŸ’¡ Note: Skipped tests have known threading issues in the source code.")
        print(f"   These would require fixing the source code (bStart flag not set in init()).")
        print()


def main():
    """Main execution function"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Generate and build unit tests for C++ code',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--use-ollama', action='store_true',
                        help='Use Ollama AI for enhanced test generation')
    args = parser.parse_args()
    
    print("="*70)
    print("Consolidated Mock & Unit Test Generator (Python + g++ Direct)")
    print("="*70)
    print()
    
    # Setup paths - read from configuration
    try:
        project_root = get_project_path()
        print(f"Using project from configuration: {project_root}")
        
        if not project_root.exists():
            print(f"âŒ Error: Project path does not exist: {project_root}")
            print("   Please update CppMicroAgent.cfg with a valid project_path")
            return 1
    except Exception as e:
        print(f"âŒ Error reading configuration: {e}")
        print("   Falling back to default: TestProjects/SampleApplication/SampleApp")
        project_root = Path("/workspaces/CppMicroAgent/TestProjects/SampleApplication/SampleApp")
    
    output_root = Path("/workspaces/CppMicroAgent/output/ConsolidatedTests")
    mock_dir = output_root / "mocks"
    test_dir = output_root / "tests"
    
    # Create directories
    mock_dir.mkdir(parents=True, exist_ok=True)
    test_dir.mkdir(parents=True, exist_ok=True)
    
    # Initialize components
    analyzer = HeaderAnalyzer(project_root)
    mock_gen = MockGenerator(mock_dir)
    test_gen = UnitTestGenerator(test_dir, mock_dir, project_root, use_ollama=args.use_ollama)
    
    # Step 1: Find all headers
    print("Step 1: Analyzing headers...")
    headers = analyzer.find_all_headers()
    header_classes = {}
    
    for header in headers:
        classes = analyzer.parse_classes_from_header(header)
        if classes:
            # Store all classes from this header - use tuple of (header_name, class_name) as key
            for class_info in classes:
                key = (header.name, class_info['class_name'])
                header_classes[key] = class_info
                print(f"  Found class: {class_info['class_name']} in {header.name}")
    
    # Step 2: Generate consolidated mocks
    print("\nStep 2: Generating consolidated mock headers...")
    for (header_name, class_name), class_info in header_classes.items():
        mock_gen.write_mock_header(class_info)
    
    # Copy common.h if it exists
    common_h = project_root / "inc" / "common.h"
    if common_h.exists():
        import shutil
        shutil.copy(common_h, mock_dir / "common.h")
        print(f"  Copied common.h")
    
    # Step 3: Process source files and generate tests
    print("\nStep 3: Generating unit tests...")
    
    # Show Ollama status prominently
    if args.use_ollama and test_gen.use_ollama:
        print("\n" + "="*70)
        print("ðŸ¤– OLLAMA AI-ENHANCED TEST GENERATION ENABLED")
        print("="*70)
        print("Each test will be:")
        print("  1. Generated using Python templates")
        print("  2. Enhanced by Ollama AI for better assertions and logic")
        print("  3. Saved with Python fallback in case of compilation issues")
        print("="*70 + "\n")
    
    source_files = analyzer.find_all_source_files()
    
    # Track which (header, class) combinations have been processed via .cpp files
    processed_classes = set()
    
    for source_file in source_files:
        print(f"\n  Processing: {source_file.name}")
        
        # Find corresponding header - try both .h and .hpp extensions
        header_name = source_file.stem + ".h"
        header_name_hpp = source_file.stem + ".hpp"
        
        # Strategy: For a source file like "tinyxml2.cpp", look for classes whose name
        # might be in that file. We'll try to match:
        # 1. Exact name match (tinyxml2 -> XMLDocument would need manual mapping)
        # 2. Any class in the matching header, but track what we actually test
        
        # Find all classes in this header that match the source file
        matching_classes = []
        for (h_name, c_name), class_info in header_classes.items():
            if h_name == header_name or h_name == header_name_hpp:
                matching_classes.append(((h_name, c_name), class_info))
        
        # Test all matching classes and track which ones we successfully generate tests for
        for (h_name, c_name), class_info in matching_classes:
            # Extract dependencies from the source file
            dependent_headers = analyzer.extract_includes_from_file(source_file)
            
            # Try to generate tests for each method - count successes
            tests_generated = 0
            for method in class_info['methods']:
                # Save the test count before
                before_count = len(test_gen.test_metadata)
                test_gen.write_test_file(source_file, class_info, method, dependent_headers)
                after_count = len(test_gen.test_metadata)
                if after_count > before_count:
                    tests_generated += 1
            
            # Only mark as processed if we actually generated tests
            if tests_generated > 0:
                processed_classes.add((h_name, c_name))
    
    # Step 3b: Process header-only files (files without corresponding .cpp)
    print("\nStep 3b: Generating tests for header-only files...")
    header_only_count = 0
    
    for (header_name, class_name), class_info in header_classes.items():
        # Skip if this class was already processed via a .cpp file
        if (header_name, class_name) in processed_classes:
            continue
        
        # This is a header-only file - find the actual header file path
        header_file = None
        for header in headers:
            if header.name == header_name:
                header_file = header
                break
        
        if header_file and class_info:
            print(f"\n  Processing header-only: {class_name} from {header_name}")
            
            # Extract dependencies from the header file itself
            dependent_headers = analyzer.extract_includes_from_file(header_file)
            
            # Generate test for each method in the header-only class
            for method in class_info['methods']:
                # Use the header file as the "source" since there's no .cpp
                test_gen.write_test_file(header_file, class_info, method, dependent_headers)
                header_only_count += 1
    
    if header_only_count > 0:
        print(f"\n  âœ… Generated tests for {header_only_count} methods in header-only files")
    
    # Show enhancement plan if Ollama is enabled
    if args.use_ollama and test_gen.enhancement_plan:
        test_gen.show_enhancement_plan()
    
    # Save metadata
    test_gen.save_metadata()
    
    # Step 4: Build and run tests
    print("\nStep 4: Building and running tests with g++...")
    builder = TestBuilder(output_root, mock_dir, project_root)
    metadata_file = output_root / "test_metadata.json"
    builder.build_and_run_all(metadata_file)
    
    print("\n" + "="*70)
    print("Generation and Testing Complete!")
    print("="*70)
    print(f"\nMock headers:          {mock_dir}")
    print(f"Unit tests:            {test_dir}")
    if args.use_ollama:
        print(f"Python backup tests:   {test_gen.python_test_dir}")
    print(f"Binaries:              {output_root / 'bin'}")
    print(f"Metadata:              {metadata_file}")
    
    if args.use_ollama:
        print(f"\nðŸ’¡ Coverage files (.gcda/.gcno) generated with --coverage flag")
        print(f"   These files are created when tests run and are used for coverage analysis")


if __name__ == "__main__":
    main()
